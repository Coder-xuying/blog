## Mysql45讲

### Mysql基础篇

![image-20210805110050300](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210805110050.png)

#### 1.基础架构

##### 1.连接器

连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。

一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限(这次连接不会生效，下次连接才会生效)

客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。



数据库里面，**长连接**是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。(连接的建立比较复杂，推荐使用长连接。就是连接之后不要断开。**比如jdbc操作的话，我们使用完之后会有一个close()的方法，这就是短连接。如果我们使用数据库连接池，就是长链接**)





MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。长连接累积下来，可能导致内存占用太大，被系统强行杀掉，也即mysql异常重启

> 怎么解决

- 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
- 可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。



##### 2.查询缓存

MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。**但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利**。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。

**MySQL 8.0 版本直接将查询缓存的整块功能删掉了**

##### 3.分析器

- 词法分析：**先确定里面的字符串的意义**，代表什么。select 是查询，表名是T。。等等
- 语法分析：会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。单词错误会报错，一般语法错误会提示第一个出现错误的位置

分析阶段判断语句是否正确，表是否存在，列是否存在等。

##### 4.优化器

经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。

**优化器是在表里面有多个索引的时候，决定使用哪个索引或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。**

优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段

##### 5.执行器

开始执行的时候，要**先判断**一下你对这个表 T **有没有执行查询的权限**，如果没有，就会返回没有权限的错误。(分析器之后也会做权限认证，precheck)

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。



#### 2.日志系统

更新流程会涉及到日志模块。redo log（重做日志）和 binlog（归档日志）。

##### redo log  

------------------------**保证事务的持久性**------------------------

**如果每次修改直接对mysql进行，那么就很慢**。因为要去磁盘进行查找，然后IO更新。于是就先写在log上，然后根据log的记录之后进行修改(其实就是 MySQL 里经常说到的 WAL 技术，**WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘**)

InnoDB 引擎就会先把记录写到 redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。





> redo log 大小

可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作

redo log满了之后又有记录进来，就需要删除之前的记录，也就是擦除

从头开始写，写到末尾就又回到开头循环写

![image-20210805112739101](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210805112739.png)

write pos，当前记录的位置，写完往后移。checkpoint是已经擦除的位置，也是往后移，可以理解成是一个环形栈空间。**write pos 和 checkpoint 之间的**是“粉板”上还空着的部分，可以用来记录新的操作。

可以看成上面的绿色的是空闲的空间。黄色的是已经写了的记录的空间。

如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得**停下来先擦掉一些记录**，把 checkpoint 推进一下。

> crash-safe

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。就是出现问题，我们可以去redo log上找存放的记录，进行一个恢复操作



log文件是固定大小的，可以把日志文件想象成一个固定大小的黑板，一边写，另一边再擦除，写完字（write pos）到擦除（check point）的位置，就是剩下的可以写的空间，这就是redo log。即使mysql崩了一段时间，有了这个之前提交的记录就不会丢失了，这个能力就是crash safe

> Flush

mysql有内存数据页和磁盘数据页，刷脏页叫flush

- 脏页：内存数据页和磁盘数据页数据不一致
- 干净页：内存数据页写入磁盘数据后，两个数据一致，就称为干净页

**什么情况下，mysql会将redo log的数据写入磁盘了？**

1. redo log写满了

2. 内存满了，有新的内存数据页要更新，淘汰一些内存数据页，如果淘汰的是脏页，就需要flush

3. 1. 这里有个问题，为什么不能直接淘汰，下次再从硬盘读出来使用redo log得到最新的结果？
   2. 答案：是因为我们认为磁盘一定是最新的，如果磁盘不一定的话，每次从磁盘取出来都需要判断redo log是否要对起做计算，这样复杂度上升了，效率也降低了

4. mysql系统空闲

5. mysql正常关闭，需要把所有的脏页都flush到磁盘

对性能的影响

1、redo log写满了，此时mysql的更新能力为0 ，不能update了，innodb需要避免这种情况

2、内存不够用了，这是mysql 的常态

- innodb用缓冲池管理内存，存在三种状态内存页，还未使用、使用了是干净页、使用了是脏页
- 淘汰策略是最久未使用（LRU）
- 如果淘汰的是干净页，直接释放就行，如果是脏页，那么要flush，所以如果一次查询淘汰的脏页太多，也会影响查询效率

InnoDB 需要有控制脏页比例的机制,避免上述影响效率的场景

innodb在不停的刷脏页，这个速度主要参考两个因素：脏页的比例 和 redo log写盘速度

**flush的特殊策略**：innodb刷脏页存在连坐的策略，即如果刷的这个脏页的下一个数据页也是脏页，会一起刷入磁盘，并且该行为会一直传递下去。



##### binlog

redo log 是 InnoDB 引擎特有的日志，而 **Server 层也有自己的日志，称为 binlog（归档日志）。**

binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。

binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

![image-20210805121603577](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210805121603.png)



> 怎样让数据库恢复到半个月内任意一秒的状态？

binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会**保存最近半个月的所有 binlog**，同时**系统会定期做整库备份**。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。



==一次误删操作的恢复==

如果我要恢复的话，首先是找到与指定时间最近一次的全量备份(整库备份)，将这个备份恢复到临时库，然后找到备份的时间点，从这个时间点开始，执行bin log里的操作，一直到误删表之前的那个时刻。

这样临时库就是跟误删之前的线上库一样了。



##### redo log 和 bin log 的理解

- redo log
  - 记录的是磁盘上数据的物理变化（记录这个页 “做了什么改动”）。
  - redo log是物理日志，物理日志与存储引擎相关
- binlog
  - 记录的是当时所执行的高级编程语言
  - bin log是逻辑日志，逻辑日志可以跨存储引擎。
  - Binlog有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，记两条，更新前和更新后都有。

#### 3.事务隔离

##### 1.隔离性和隔离级别

隔离性：

- 当数据库上有多个事务同时执行的时候，就可能出现**脏读**（dirty read）、**不可重复读**（non-repeatable read）、**幻读（**phantom read）的问题

隔离级别：

- 读未提交（read uncommitted)：一个事务还没提交时，它做的变更就能被别的事务看到
- 读提交（read committed）：一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在**启动时**看到的数据是一致的。
- 串行化（serializable ）：读取前锁定所有要读读取的数据，当前事务提交前其他事务不允许修改

> 分析

![image-20210806142440115](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806142440.png)

不同隔离级别下得到的值：

- RU。V1 = 2. V2 =V3 =2 。B没有提交，但是结果还是被A看到了
- RC。V1 = 1，V2 =2.V3 = 2。 因为B没提交，所以V1的值还是之前看到的那个。提交之后，这个值才能被A看到
- RR。V1 = 1，V2 = 1，V3 = 2。事务在执行期间看到的数据前后必须是一致的。
- 串行化。V1 = V2 =1 。V3 =2 。但是事务B执行“将 1 改成 2”的时候，**会被锁住**。必须事务A提交之后才能继续执行

实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。**在“可重复读”隔离级别下，这个视图是在事务启动时创建的**，整个事务存在期间都用这个视图。**在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的**。

**注意**：读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

总结来说，存在即合理，每种隔离级别都有自己的使用场景，你要根据自己的业务情况来定。





##### 事务隔离的实现

> 可重复读

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志（undo log）里面就会有类似下面的记录。

![image-20210806143353025](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806143353.png)

当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。

同一条记录在系统中可以存在多个版本，就是**数据库的多版本并发控制（MVCC）**。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。



同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。

回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。

什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。

**最好不用长事务**：

- 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

##### 事务的启动方式

- 显示启动事务，begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。
- set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接
- 

#### 4.索引

##### 常见的索引类型

- 哈希表 （key-value）：把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。多个key经过hash 的值可能相同，使用拉链法处理。
  - 哈希表这种结构适用于只有**等值查询**的场景
- 有序数组：而有序数组在**等值查询和范围查询场景**中的性能就都非常优秀。
  - 有序数组索引只适用于**静态存储引擎**：（更新数据需要挪动很多数据，开销大）。比如你要保存的是 2017 年某个城市的所有人口信息，这类不会再修改的数据。
- 二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。**使用多叉树**。“N 叉”树中的**“N”取决于数据块的大小。**
  - 以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。
- 跳表、LSM 树等数据结构也被用于引擎设计

##### InnoDB的索引模型

InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。**每一个索引在 InnoDB 里面对应一棵 B+ 树。**

![image-20210806144636639](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806144636.png)

主键索引的叶子节点存的是整行数据。主键索引也被称为聚簇索引（clustered index）。

非主键索引的叶子节点内容是主键的值，非主键索引也被称为二级索引（secondary index）。

基于**非主键索引的查询需要多扫描一棵索引树**。如果我们要查的值是主键，就可以不用回表重新扫描，这就是**索引覆盖**。因此，我们在应用中应该尽量使用主键查询。



> 索引维护

B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。根据上图的，如果插入700，只需要在R5的后面插入一个新的值，如果插入的是400就需要逻辑上挪动后面的数据。还可能R5的数据页慢了，B+就需要申请一个新的数据页，然后挪动部分数据过去，这就是**页分裂**。

页分裂：

- 性能会有影响
- 空间利用率降低，原来一个页的数据，现在要放到两个页

有分裂就有合并，如果删除数据之后，利用率比较低，会将数据页做一个合并。



> **自增主键**：

插入新纪录可以不指定ID的值，获取当前最大的ID的值+1.就跟上面直接插入700的例子一样，使用自增主键的话，后面添加新的记录，只需要在最后一个数据页里面**追加**，不需要挪动其他数据。如果这个数据页慢了，就直接申请新的页，将后面的值添加到新的数据页里面，然后更新上面的索引页。**如果申请的数据页比较多，那么索引页还是会进行一个分裂的操作。**

显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。所以，从性能和存储空间方面考量，**自增主键往往是更合理的选择**。因为业务逻辑的字段**不容易保证有序的插入**。而且如果是字符串类型的，一般需要用20个字节。整形数据做主键只需要4个字节，或者长整型的8个字节。就可以一个页放更多的数据了。

有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：

- 只有一个索引
- 该索引必须是唯一索引。



##### 最左前缀匹配

**联合索引**：

![image-20210806151823599](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806151823.png)

这是一个name和age的联合索引，顺序是按照name来排列的。然后当name相同，才是按照age进行一个排序。



如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是"where name like ‘张 %’"。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。

**只要满足最左前缀，就可以利用索引来加速检索**。最左前缀可以是联合索引的最左 N 个字段

>在建立联合索引的时候，如何安排索引内的字段顺序。

所以当已经有了 (a,b) 这个联合索引后，**一般就不需要单独在 a 上建立索引了**。

第一原则是，**如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的**。

##### 索引下推

**这里建立了name和age的联合索引**

select * from tuser where name like '张%' and age=10 and ismale=1;

你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。

MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

 **其实就是索引空间里面存放着有name和age，然后查找name之后，直接用age的值进行比较，然后不满足的，就不回表比对**

![image-20210806152626965](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806152627.png)



#### 5.各种锁

**DML**：数据库管理语言，如select、update这些

**DDL**：数据库定义语言，这个是对表的操作，比如给表加字段的命令

##### 全局锁

全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的语句会被阻塞(DML、DDL)。

全局锁的典型使用场景是，做**全库逻辑备份**。在备份过程中整个库完全处于只读状态。这是比较危险的：

- 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；
- 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。



> 我们可以不用加锁，也能实现的备份

在可重复读隔离级别下开启一个事务。能够拿到一致性视图

官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。

但是有的数据库引擎不支持事务，或者这个隔离级别。所以需要使用FTWRL。



##### 表锁

MySQL 里面表级别的锁有两种：一种是**表锁**，一种是**元数据锁（meta data lock，MDL)**。

表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。

表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。



**另一类表级的锁是 MDL（metadata lock)**

当对一个表做增删改查操作的时候(DML语句)，加 MDL 读锁；当要对**表做结构变更**操作的时候(DDL语句)，加 **MDL 写锁**。

- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。



![image-20210818183031032](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210818183031.png)

我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。之后所有要在表 t 上新申请 MDL 读锁的请求**也会被 session C 阻塞**。就都被锁住，等于这个表现在完全不可读写了。

==为什么C锁住了，后面的读锁也都锁住==是因为申请MDL锁的时候，会形成一个队列，队列中**写锁的获取的优先级高于读锁**-。于是写锁如果等待了，后面的读锁就抢不过写锁，也会阻塞住。

> ==实际操作中，必须把D提交了，C才能继续执行，怎么会出现插队的情况？？？== 

是因为mysql的online ddl机制，DDL(更改表结构的语句)，ddl执行的时候，如果锁住表就会严重影响性能，不锁表，又不好解决DML语句的影响，于是有了上面这个机制。 这个会让DDL有个锁降级的过程，如下：

- 事务拿到MDL写锁
- 降级成MDL读锁
- 申请一块空间，开始改变表结构，填数据(做DDL操作)
- 升级为MDL写锁，然后把上述表替换之前的表
- 释放MDL写锁

可以知道，在锁降级之后，事务D就会拿到MDL的读锁，因此，我们升级回MDL写锁，就会被阻塞住。

> 如何安全的给小表加字段

![image-20210818184217134](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210818184217.png)



##### 行锁

MySQL 的行锁是在引擎层由各个引擎自己实现的。不是所有的引擎都支持行锁，MyISAM 引擎就不支持行锁，意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。



在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时(commit)才释放。这个就是**两阶段锁协议**。



知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行**，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。**



有一个业务的操作：

- 1 从顾客 A 账户余额中扣除电影票价；
- 2 给影院 B 的账户余额增加这张电影票价；
- 3 记录一条交易日志。

这时候另一个事务是，顾客C要在影院B买票，那么就和语句2产生锁冲突了，所以我们要把语句2放在最后处理，也就是事务语句位置为3,1,2.

> 死锁和死锁检查

![](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210818211253.png)

上面会出现死锁的情况，出现死锁有两种解决办法：

- 直接进入等待，一直到超时，超时时间可以通过参数 innodb_lock_wait_timeout 来设置。
- 发起死锁检查。发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。

在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。又不可能直接把这个时间设置太小，比如 1s。这样的话如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。

**一般都采用第二种方式**：

加入这样一个过程，每个事务被锁的时候，都要查看它依赖的线程有没有被别人锁住，然后判断是否出现死锁。如果是所有事务都更行同一行(热点数据),那么每个新来的线程都会被堵住，然后判断是不是自己导致死锁，假设有1000个并发的，就是1000*1000 = 100w的检测量级。这并没有发送死锁，但是会消耗大量的CPU资源。



>怎么解决由这种热点行更新导致的性能问题呢？

1.如果确定业务一定不会出现死锁，可以关闭死锁检测。业务设计的时候一般不会把死锁当做一个严重错误，毕竟**出现死锁了，就回滚**，然后通过业务重试一般就没问题了，这是**业务无损的**。而**关掉死锁检测**意味着可能会**出现大量的超时**，这是**业务有损的**。

2.控制并发度。并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现。**对于相同行的更新，在进入引擎之前排队。**



### 12.Mysql实践篇

#### 1、order by 

> select a,b,c where a = "xx" order by b limit 1000 
>
> 假设有4000条数据满足a="xx"

**全字段排序**

执行过程：

1. 初始化sort_buf ，放入a,b,c三个字段
2. 选取索引a，拿到对应的主键id，然后去主键索引树种拿出整行数据
3. 一直取数据，直到索引a!=xx，
4. 在sort_buf中根据b进行排序，返回前1000行数据

排序过程，可能在内存中进行，也可能通过外部辅助文件排序（一般为归并排序）

性能：

- 扫描行数主键索引4000次
- 外部排序文件假设为12





**rowid排序**

上面存在一个问题，比如需要放入sort_buf的字段太多，内存能放的条数就少了，会分成很多个临时文件，影响效率

`SET max_length_for_sort_data = 16;` 假设用了这个之后，内存放不小上面三个字段了，这时候，就只会把b和主键id放进去，

执行过程就会变成：

1. 初始化sort_buf ，放入b和主键id字段
2. 选取索引a，拿对应的主键id去主键索引树中，获取b数据
3. 一直取数据，直到索引a!=xx
4. 在sort_buf中根据b进行排序，返回前1000的主键id
5. 再去主键索引树中，取对应的a,b,c的数据返回

性能：

- 扫描行数主键索引4000次+1000次
- 外部排序文件会小于12，放入的字段变少了

原因很容易理解，内存比较大，mysql更倾向于全字段排序，减少io操作



**建立a 和b的联合索引**

执行过程：

1. **不会初始化sort_buf **
2. 直接使用ab的联合索引，然后取出前1000个主键id
3. 去主键索引树里取出对应的数据返回

性能：

- 扫描行数主键索引1000ci
- 查询过程不需要临时表，也不需要排序

如果使用abc的联合索引，会走到覆盖索引直接返回，不会回表查询



#### 2、自增主键

自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂

**提问：自增主键是连续递增的吗？**



表的定义里面有一个AUTO_INCREMENT ,这个表示下一个自增值

- myISAM 自增值保存在数据文件中
- innodb自增值
  - 在5.7版本及以前都保存在内存中，不会持久化，也即**mysql重启之后会自己寻找最大的id**
    - 如果数据里最大的id是10，自增值是11，我们删除10的数据，然后重启实例，自增值就会变成10
  - 8.0版本，自增值的变更会记录在redo log里，重启后依靠redo log恢复之前的值

> 自增值修改机制

插入数据时，自增值行为为：

- 如果id指定为0、null或者未知值，那么直接用AUTO_INCREMENT 数据填入自增字段
- 如果id指定了具体指，使用语句里指定的值，假如指定值为X，AUTO_INCREMENT 为Y
  - 如果X>= Y，需要把AUTO_INCREMENT修改为新的自增值
    - 新自增值的算法如下：
      - auto_increment_offset 和 auto_increment_increment 是两个系统参数，分别用来表示自增的初始值和步长，默认值都是 1  （有些场景如双写架构，auto_increment_increment设置为2，然后自增id一个为奇数、一个为偶数，避免主键冲突）
      - 算法：用初始值+步长，找到第一个大于X的值，作为新的自增值
      - 如果都为1，就是直接准备插入的值+1就行
  - 如果X<Y，AUTO_INCREMENT 不变



> 讨论下 初始值和步长都为1的情况下，自增值为什么也不一定是连续的？

3种原因：

- 当我们插入一条新数据的时候，自增值会先分配id，然后修改自增值，这时候再进行插入数据操作，如果有报错，那么这个时候自增值+1了，但是中间那条数据就没有了
- 如果事务回滚，也会有类似的现象
- 一个语句插入多条数据（insert…select），同一个语句申请id会每次都翻倍。比如插入4条数据，第一次会分配1个id，第二次分配2个id，第三次分配4个，但是只用4个，也就有3个id浪费了，也不会回滚了
  - todo：自增锁的优化 没记录进去

为什么mysql不让自增值回滚了，主要是为了性能

- 如果自增值回滚，事务A插入了数据id=2，事务B插入数据id=3，事务A回滚了，自增值也跟着回滚id=2，那么其他事务再来插入数据，此时表里有id=3的数据，那么就会有异常。解决这个异常就需要做额外的操作：
  - 方法一： 每次申请id之前，都判断是否存在id，如果存在就跳过，成本太高
  - 方法二：自增id的锁范围增大，必须等事务执行完成提交，下一个事务才能申请id，这个并发能力就要下降