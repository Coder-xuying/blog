### 面经

#### 中至数据

1. 5*5个人，每个赛道5个人进行比赛，怎么最快选出前三名？
2. 如果是n*5个人？
3. 给n个数（n比较大）怎么选出中位数？（我说快排的过程中选出中位数）
4. 快排和冒泡的效率不同，时间复杂度怎么计算的
5. 对快排的理解
6. 一个（资深的、优秀的）程序员你觉得是什么样子的？你准备怎么成为这种程序员？



#### 腾讯云（子公司）。

翻译，人工同传，武汉做工程化

##### 一面

1.mini-sls项目整个链路的通信模式。

2.mini-sls怎么对接这么多个不同的物流商的

3.（每个供应商对应一个接口）说的是哪种设计模式

4.做项目完，你的成长有哪些

5.怎么做的分库分表，用什么字段做的sharding key

6.用hash做这个分库分表有什么缺点（我说了个水平扩展的）

7.用什么能解决上面那个缺点了（回答一致性hash），给我讲讲一致性

8.项目里面表用的主键是什么字段（id做主键），为什么要这样设计

9.唯一索引和主键索引的区别

10.mysql的隔离级别

11.mysql的不可重复读和幻读是什么

12.mysql哪些隔离级别来解决上面的两个问题

13.mvcc知道吗？使用什么日志来实现mvcc的

14.mvcc的不同事务之间的视图可见性说一下

15.项目里面为什么要做分布式锁，怎么做的？（setnx+过期），这种实现方式有什么缺点？（扯了一点集群）

16.Codis是你们自研的吗？这个和redis cluster的区别。 （瞎扯了点架构），这个是没有用哨兵吗？（瞎扯，我说放在客户端那边做监控，他问为什么这样设计，有什么好处）

17.幂等性的理解（）

18.kafka在项目里面用来做什么？（说了异步操作和异常重试机制）

19.虚拟内存和物理内存关系

20.一个进程的多个线程之间内存关系（我应该说错了，我说是独立的，他就问那线程怎么通信的）

21.问了我读研的研究方向（简单介绍了一下我做的是啥）

22.有没有抓过包？

23.怎么用包来判断这个tcp的开始和结束

24.怎么用包找到我需要的请求的开始和结束（扯了个目的地址）

25.只用目的地址就能定位吗？（）

26.算法题：leetcode442

##### 二面

1.mini-sls的履约模块有有什么难点？

2.消费的时候怎么保证向下游下物流单时的一个幂等性（重复消费的问题）？

3.下的单子在整个链路中总共有多少种状态？

4.重试机制中，如果有两个work去重试下单，怎么处理的？（和问题2一样，问题2我搪塞过去了）。 我回答当时没有处理这种情况

5.上面的这个问题会出现吗？  我说除非定时任务设置不合理，那你们用的saturn能够避免这种情况吗？ 不知道

6.分库分表怎么做的？

7.你们的分表200张表是固定的吗？以后做扩展怎么办？ 扯了一下一致性hash算法的原理，怎么加节点

8.做数据迁移的时候要注意什么？ 集群需要停机吗？ 回答错了，后面扯回来了（）

9.实际业务中肯定不能停机，有什么方法可以让数据迁移的时候，不影响业务的使用

10.大屏项目，计算的时候服务挂了怎么办？

11.统计均值，服务挂了，会怎么样？有问题，那怎么解决

12.大屏项目的mysql存什么内容？这部分数据用来干什么？

13.mysql里面的数据会定时清理吗？

14.golang的进程、线程、协程的区别？协程和线程的最大区别

15.golang里面GMP的理解，G映射到操作系统里是什么东西？GMP中的M和P的个数，为什么M要和操作系统的核心数相同

16.内核态和用户态最大的区别？

17.为什么要区分内核态和用户态？

18.如果能够操作内核态，有什么问题？

19.go里面的channel的类型？

20.什么场景下用有缓冲的，什么场景下用无缓冲的？

21.https为什么能够保证安全？怎么加密的？为什么要两种加密方式一起用？

22.中间人攻击？数字证书怎么防止中间人攻击的？

23.websocket了解吗？基于哪层协议做的？自我怀疑--先答的tcp后面改成错误答案udp了

24.tcp和udp的区别

25.滑动窗口，拥塞控制

26.websocket用在什么场景？

27.websocket的优点，相比于http？

28.介绍一下红黑树，一般用在什么场景，为什么用在搜索的场景？

#### boss直聘 一面



#### pdd

##### 二面

1.hyperloglog解决什么问题？

2.监控系统的流程

3.codis集群的架构

4.redis cluster 往下深入一些

5.怎么做的分库分表

6.索引失效的几种情况

7.redis的数据类型

8.性别字段需要建索引吗，建了查询的时候会走索引吗？

9.跳表的结构，跳表和树的区别

10.kafka的架构

11.如何保证kafka的顺序消费（回答指定partition），单个partition开多线程去消费，怎么保证消费的顺序？

12.CAP理论，zookeeper是cp还是ap？

13.tcp三次握手，四次挥手。

- 学一下sharding jdbc

### 技术面

#### 1.Mysql怎么知道一条SQL如何执行的

通过`explain`命令我们可以学习到该条SQL是如何执行的，随后解析explain的结果可以帮助我们使用更好的索引，最终来优化它！



> 字段

- id: 查询中执行SELECT子句或操作表的**顺序** （select XXX from (select xx)） 嵌套的第一个select就是1，后面的select就是2
- select_type，表示select查询的类型 。
  -  **SIMPLLE**：简单查询，该查询不包含 UNION 或子查询
  - **PRIMARY**：如果查询包含UNION 或子查询，则**最外层的查询**被标识为PRIMARY
  - 等等
- table：表示正在访问哪个表
- type
  - ALL：全表扫描
  - index：全索引扫描
  - range：**范围扫描**，有限制的索引扫描
  - ref：一种索引访问，也称索引查找，它返回所有匹配某个单个值的行。
  - eq_ref：使用这种索引查找，最多只返回一条符合条件的记录。在使用唯一性索引或主键查找时会出现该值，非常高效。
  - ...这个比较好用
- possible_keys   这一列显示查询**可能**使用哪些索引来查找
- key：这一列显示MySQL**实际**决定使用的索引。如果没有选择索引，键是NULL
- rows：这一列显示了**估计**要找到所需的行而要读取的行数，这个值是个估计值，原则上值越小越好。



可以用来进行检查sql语句，优化语句



#### 2.为什么MySQL的索引要使用B+树，而不是其它树？比如B树？

> 为什么不用hash。

 hash在查询一条数据的时候，效率是最快的。但是他有缺点。内存限制。我们没办法直接把hash存储的索引一次性从硬盘全部读取到内存中。就需要频繁的进行磁盘IO读写。然后查找。

索引就可以用树结构存储

**而且经常查询的是范围值**，

> 为什么不用红黑树或者二叉排序树

二叉排序树，在有序的情况下，可能会退化成数组。就和hash一样了。反正就是即使用二叉排序树，数据量很多的情况下，就会比较深



B树的优势就出现了 。

可以每次往内存里面加载B树的一个节点，开始搜索。

> 为什么不用B树

select的时候一般都是查找多个数据。如果是B树的话，需要做局部的遍历，可能会跨层访问，比较麻烦。

而B+树，在B树的基础上，又**把数据全部存放到叶子节点上了，就很容易实现范围遍历**

**B+树的设计可以允许数据分批加载，同时树的高度较低**，提高查找效率。



#### 3.nginx的负载均衡策略

- 轮询（默认）：每个请求按时间顺序逐一分配到不同的后端服务器

- 指定权重：指定轮询几率，**weight和访问比率成正比**，用于后端服务器性能不均的情况。

  - ```java
    upstream backserver { 
    server 192.168.0.14 weight=8; 
    server 192.168.0.15 weight=10; 
    } 
    ```

- 根据IP分配。 ip_hash  -- 同一个客户端的请求都转发到同一个服务器，保证了session的统一性

  - ```java
    upstream balanceServer {
        ip_hash; # 指定负载均衡策略为ip_hash
        server localhost:8081;
        server localhost:8082 backup;
        server localhost:8083 max_fails=3 fail_timeout=20s;
        server localhost:8084 weight=2;
    }
    ```

- 最少连接（least_conn）

  - 策略是把请求转发给连接数较少的后端服务器。前面的轮询策略是把请求平均地转发给集群中的每个后台服务器，使得它们的负载大致相同，但是有些请求可能占用的时间会很长，可能导致所在的后端负载过高。这种情况下选用least_conn策略就能达到更好的负载均衡效果。

- 响应时间（fair）：fair策略是一种**第三方策略，**需要安装第三方插件

  - 是按照服务器的响应时间来分配请求，响应时间短的优先分配。

- 根据URL分配（url_hash）：按照url的hash结果来分配请求，使得每个url定向到同一个后端服务器，要配合缓存命中来使用。**(也是第三方策略)**

  - 同一个资源多次请求，可能会到达不同的服务器上，导致不必要的多次下载，缓存命中率不高，以及一些资源时间的浪费。而使用url_hash的话，就可以使得同一个url（也就是同一个资源请求）到达同一台服务器，一旦缓存住了资源，再次收到请求，就可以从缓存中读取。



#### 4、锁



Automic 底层

![image-20210801223721784](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210801223721.png)



#### 5、mysql读写分离怎么实现的

先在主mysql上配置主节点，再去从mysql上配置从节点，和主节点的host。进行主从复制

之后使用中间件mycat。配置主节点和从节点的地址，然后修改balance属性，进行读写分离的配置。

> mycat 原理

Mycat 的原理中最重要的一个动词是“拦截”，它**拦截了用户发送过来的 SQL 语句**，首先对 SQL 语句做了一些特定的分析：如分片分析、路由分析、读写分离分析、缓存分析等，然后将此 SQL 发往后端的真实数据库，并将返回的结果做适当的处理，最终再返回给用户。

> balance属性配置读写分离的类型

```
（1）balance="0", 不开启读写分离机制，所有读操作都发送到当前可用的 writeHost 上。

（2）balance="1"，全部的 readHost 与 stand by writeHost 参与 select 语句的负载均衡，简单的说，当双主双从模式(M1->S1，M2->S2，并且 M1 与 M2 互为主备)，正常情况下，M2,S1,S2 都参与 select 语句的负载均衡。

（3）balance="2"，所有读操作都随机的在 writeHost、readhost 上分发。

（4）balance="3"，所有读请求随机的分发到 readhost 执行，writerHost 不负担读压力
```



#### 6、海量数据怎么排序



归并排序的过程

![](https://pic3.zhimg.com/v2-cdda3f11c6efbc01577f5c29a9066772_b.webp)

排序算法：归并排序 - 程序员囧辉的文章 - 知乎 https://zhuanlan.zhihu.com/p/36075856





**外排序**
　　传统的排序算法一般指内排序算法，针对的是数据可以一次全部载入内存中的情况。但是面对海量数据，即数据不可能一次全部载入内存，需要用到外排序的方法。外排序采用分块的方法（分而治之），首先将数据分块，对块内数据按选择一种高效的内排序策略进行排序。然后采用归并排序的思想对于所有的块进行排序，得到所有数据的一个有序序列。

　　例如，考虑一个1G文件，可用内存100M的排序方法。首先将文件分成10个100M，并依次载入内存中进行排序，最后结果存入硬盘。得到的是10个分别排序的文件。接着从每个文件载入9M的数据到输入缓存区，输出缓存区大小为10M。对输入缓存区的数据进行归并排序，输出缓存区写满之后写在硬盘上，缓存区清空继续写接下来的数据。对于输入缓存区，当一个块的9M数据全部使用完，载入该块接下来的9M数据，一直到所有的9个块的所有数据都已经被载入到内存中被处理过。最后我们得到的是一个1G的排序好的存在硬盘上的文件。



1TB数据使用32GB内存如何排序
　　①、把磁盘上的1TB数据分割为40块（chunks），每份25GB。（注意，要留一些系统空间！）
　　②、顺序将每份25GB数据读入内存，使用quick sort算法排序。
　　③、把排序好的数据（也是25GB）存放回磁盘。
　　④、循环40次，现在，所有的40个块都已经各自排序了。（剩下的工作就是如何把它们合并排序！）
　　⑤、从40个块中分别读取25G/40=0.625G入内存（40 input buffers）。
　　⑥、执行40路合并，并将合并结果临时存储于2GB 基于内存的输出缓冲区中。当缓冲区写满2GB时，写入硬盘上最终文件，并清空输出缓冲区；当40个输入缓冲区中任何一个处理完毕时，写入该缓冲区所对应的块中的下一个0.625GB，直到全部处理完成。



### HR面/主管面

#### 你有什么缺点和优点

缺点：

- 缺少实际的工作经验。不过我觉得自己的学习能力不错，之后入职，应该可以比较快的通过学习，适应岗位。
- 做决定不够果断，有选择困难症
- 有利他的行为，就是有时候自己的事还没做完，别人请求帮忙，不太会拒绝，就导致之后做事剩下的时间变短，给自己增加了一些压力
- 记性不是很好，有时候容易忘事，所以我都是会写一些便签

优点：

- 情绪比较稳定吧，不轻易发泄怒火，生活中的事情最后都能看的比较淡。
- 比较信守诺言，答应别人的事都会竭力完成
- 适应能力比较强。

#### 你如何看待加班

我觉得如果是工作需要，我会比较积极的加班，同时，我也会提高工作效率，减少不必要的加班

#### 你对薪资的要求？

并没有很硬性的要求，我觉得贵公司应该会针对我前几轮的面试评价，给出一份合理的薪资

#### 在五年的时间内，你的职业规划？

在技术方面有一定的积累。按照公司的上升渠道进行一个合理发展



#### 遇到过最困难的事情



#### 最有挑战的事情？ (思考)



### 反问环节整理



反问：

1. 我想了解一下公司的培训机制和学习机制
2. 公司的晋升路径是怎样的，或者说渠道

公司对待新人有哪些培训
工作的主要内容和职责
部门的工作氛围，加班情况
在公司的一天是如何度过的？
对于未来加入这个团队，你对我的期望是什么？

您认为优秀的员工应该具备哪些品质呢？（特质）

在入职前，有哪些需要我学习和准备的呢？（个人）

公司对这个目前所在团队的发展定位是什么样的？（团队）

您觉得我对于胜任这份工作目前还有哪些差距？（赋能）



# 项目

## 博客

### 介绍

后面还用redis做了一个文章点击量和阅读排行榜的一个功能。使用的有序集合zset，key是文章的id，然后value就是 文章的点击量，所有的这些数据放在一个有序集合点击量里面。然后在访问文章请求的的拦截器里面，加上了redis的操作，有人点击就让redis取出对应id文章的点击量的值让它+1，然后把这个值刷新进数据库进行保存。

排行榜就是用zset中用函数zrevrange取出点击量最多的20篇作为热门文章，显示在主页的热门文章列表里面。



- 异常的处理：  @ExceptionHandler(异常的类)
  - 创建了一个全局异常处理类，使用aop横切进去返回异常的处理信息。
- 跨域的处理：
  - 首先实现了一个CorsConfig的配置类
  - 因为我们是自定义了一个jwtFilter来过滤请求，因此，跨域问题在经过滤器之前做了一个预处理.进行跨域
- 统一结果封装：
  - 自定义了Result类，里面封装根据不同code代码，返回不同信息的方法 。 类包含三个参数code,msg,data
- 实体类里面的校验：@Validated
  - 使用了JSR303的数据校验，用在实体类上加注解，controller层进行校验
  - 对校验不通过的信息使用aop处理异常，返回信息
- token我们是放在返回报文里的authorization字段里面

> 数据库

项目主要是用了一个mybatis-Plus代码生成器 生成和数据库对应的实体类和基本的增删改查操作，使用pageHelper这个插件实现的分页功能。

表的话，设计的比较简单 

- 一个是用户表：账号，密码，用户的基本信息
- 一个是博客表：标题，内容，访问量，用户ID

#### shiro

##### 后端

关闭了shiro自带的session，就不再能通过session方式登录shiro。采用jwt凭证的登录

> 后台  -shiro +jwt 会话共享

主要就是doGetAuthenticationInfo登录认证这个方法，可以看到我们通过jwt获取到用户信息，**判断用户的状态**

然后把jwt这个字符串，放进Authorization。  

退出的话只需要在使用shiro的logout方法，就可以将会话取消。



> shiro的三个类

subject记录了当前操作用户,外部程序通过subject进行认证授，而subject是通过SecurityManager安全管理器进行认证授权

SecurityManager 即安全管理器，对全部的subject进行安全管理，它是shiro的核心，负责对所有的subject进行安全管理。

realm是shiro进行登录或者权限校验的逻辑所在 .认证授权校验的相关的代码。

![image-20210804155355082](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210804155355.png)

##### 前端

vue+route+xx(markdown语法) + store

> store

用来储存全局的变量。就是这里面的数据，你点击连接之后，都能获取到

要把参数发送到所有的组件，而且这个改变之后，其他的组件能够及时的收到。比如token，还有userinfo的信息。存在localStorage里面或者sessionStorage

里面有get和set方法

### Token 认证的优势

相比于 Session 认证的方式来说，使用 token 进行身份认证主要有下面三个优势：

#### 1.无状态

token 自身包含了身份验证所需要的所有信息，使得我们的服务器不需要存储 Session 信息，这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。但是，也正是由于 token 的无状态，也导致了它最大的缺点：当后端在token 有效期内废弃一个 token 或者更改它的权限的话，不会立即生效，一般需要等到有效期过后才可以。另外，当用户 Logout 的话，token 也还有效。除非，我们在后端增加额外的处理逻辑。

#### 2.有效避免了CSRF 攻击

**CSRF（Cross Site Request Forgery）**一般被翻译为 **跨站请求伪造**，属于网络攻击领域范围。相比于 SQL 脚本注入、XSS等等安全攻击方式，CSRF 的知名度并没有它们高。但是,它的确是每个系统都要考虑的安全隐患，就连技术帝国 Google 的 Gmail 在早些年也被曝出过存在 CSRF 漏洞，这给 Gmail 的用户造成了很大的损失。

那么究竟什么是 **跨站请求伪造** 呢？说简单用你的身份去发送一些对你不友好的请求。举个简单的例子：

小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了10000元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求,也就是通过你的 Cookie 向银行发出请求。

```
<a src=http://www.mybank.com/Transfer?bankId=11&money=10000>科学理财，年盈利率过万</>
```

导致这个问题很大的原因就是： Session 认证中 Cookie 中的 session_id 是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。

**那为什么 token 不会存在这种问题呢？**

我是这样理解的：一般情况下我们使用 JWT 的话，在我们登录成功获得 token 之后，一般会选择存放在 local storage 中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个 token,这样就不会出现 CSRF 漏洞的问题。因为，即使有个你点击了非法链接发送了请求到服务端，这个非法请求是不会携带 token 的，所以这个请求将是非法的。

但是这样会存在 XSS 攻击中被盗的风险，为了避免 XSS 攻击，你可以选择将 token 存储在标记为`httpOnly` 的cookie 中。但是，这样又导致了你必须自己提供CSRF保护。

具体采用上面哪两种方式存储 token 呢，大部分情况下存放在 local storage 下都是最好的选择，某些情况下可能需要存放在标记为`httpOnly` 的cookie 中会更好。

#### 3.适合移动端应用

使用 Session 进行身份认证的话，需要保存一份信息在服务器端，而且这种方式会依赖到 Cookie（需要 Cookie 保存 SessionId），所以不适合移动端。

但是，使用 token 进行身份认证就不会存在这种问题，因为只要 token 可以被客户端存储就能够使用，而且 token 还可以跨语言使用。

#### 4.单点登录友好

使用 Session 进行身份认证的话，实现单点登录，需要我们把用户的 Session 信息保存在一台电脑上，并且还会遇到常见的 Cookie 跨域的问题。但是，使用 token 进行认证的话， token 被保存在客户端，不会存在这些问题。

### Token 认证常见问题以及解决办法

#### 1.注销登录等场景下 token 还有效

与之类似的具体相关场景有：

1. 退出登录;
2. 修改密码;
3. 服务端修改了某个用户具有的权限或者角色；
4. 用户的帐户被删除/暂停。
5. 用户由管理员注销；

这个问题不存在于 Session 认证方式中，因为在 Session 认证方式中，遇到这种情况的话服务端删除对应的 Session 记录即可。但是，使用 token 认证的方式就不好解决了。我们也说过了，token 一旦派发出去，如果后端不增加其他逻辑的话，它在失效之前都是有效的。那么，我们如何解决这个问题呢？查阅了很多资料，总结了下面几种方案：

- **将 token 存入内存数据库**：将 token 存入 DB 中，redis 内存数据库在这里是是不错的选择。如果需要让某个 token 失效就直接从 redis 中删除这个 token 即可。但是，这样会导致每次使用 token 发送请求都要先从 DB 中查询 token 是否存在的步骤，而且违背了 JWT 的无状态原则。
- **黑名单机制**：和上面的方式类似，使用内存数据库比如 redis 维护一个黑名单，如果想让某个 token 失效的话就直接将这个 token 加入到 **黑名单** 即可。然后，每次使用 token 进行请求的话都会先判断这个 token 是否存在于黑名单中。
- **修改密钥 (Secret)** : 我们为每个用户都创建一个专属密钥，如果我们想让某个 token 失效，我们直接修改对应用户的密钥即可。但是，这样相比于前两种引入内存数据库带来了危害更大，比如：1⃣️如果服务是分布式的，则每次发出新的 token 时都必须在多台机器同步密钥。为此，你需要将必须将机密存储在数据库或其他外部服务中，这样和 Session 认证就没太大区别了。2⃣️如果用户同时在两个浏览器打开系统，或者在手机端也打开了系统，如果它从一个地方将账号退出，那么其他地方都要重新进行登录，这是不可取的。
- **保持令牌的有效期限短并经常轮换** ：很简单的一种方式。但是，会导致用户登录状态不会被持久记录，而且需要用户经常登录。

对于修改密码后 token 还有效问题的解决还是比较容易的，说一种我觉得比较好的方式：**使用用户的密码的哈希值对 token 进行签名。因此，如果密码更改，则任何先前的令牌将自动无法验证。**

#### 2.token 的续签问题

token 有效期一般都建议设置的不太长，那么 token 过期后如何认证，如何实现动态刷新 token，避免用户经常需要重新登录？

我们先来看看在 Session 认证中一般的做法：**假如 session 的有效期30分钟，如果 30 分钟内用户有访问，就把 session 有效期被延长30分钟。**

1. **类似于 Session 认证中的做法**：这种方案满足于大部分场景。假设服务端给的 token 有效期设置为30分钟，服务端每次进行校验时，如果发现 token 的有效期马上快过期了，服务端就重新生成 token 给客户端。客户端每次请求都检查新旧token，如果不一致，则更新本地的token。这种做法的问题是仅仅在快过期的时候请求才会更新 token ,对客户端不是很友好。
2. **每次请求都返回新 token** :这种方案的的思路很简单，但是，很明显，开销会比较大。
3. **token 有效期设置到半夜** ：这种方案是一种折衷的方案，保证了大部分用户白天可以正常登录，适用于对安全性要求不高的系统。
4. **用户登录返回两个 token** ：第一个是 acessToken ，它的过期时间 token 本身的过期时间比如半个小时，另外一个是 refreshToken 它的过期时间更长一点比如为1天。客户端登录后，将 accessToken和refreshToken 保存在本地，每次访问将 accessToken 传给服务端。服务端校验 accessToken 的有效性，如果过期的话，就将 refreshToken 传给服务端。如果有效，服务端就生成新的 accessToken 给客户端。否则，客户端就重新登录即可。该方案的不足是：1⃣️需要客户端来配合；2⃣️用户注销的时候需要同时保证两个 token 都无效；3⃣️重新请求获取 token 的过程中会有短暂 token 不可用的情况（可以通过在客户端设置定时器，当accessToken 快过期的时候，提前去通过 refreshToken 获取新的accessToken）。

### 总结

JWT 最适合的场景是不需要服务端保存用户状态的场景，比如如果考虑到 token 注销和 token 续签的场景话，没有特别好的解决方案，大部分解决方案都给 token 加上了状态，这就有点类似 Session 认证了。





### 前端

vue+route+xx(markdown语法) + store

> store

用来储存全局的变量。就是这里面的数据，你点击连接之后，都能获取到

要把参数发送到所有的组件，而且这个改变之后，其他的组件能够及时的收到。比如token，还有userinfo的信息。存在localStorage里面或者sessionStorage

里面有get和set方法

> axios

在axios里做一个前置拦截，添加请求头。。

axios.js 

配置默认请求url和后置拦截。根据获取的返回结果的code进行拦截。

如果是200就成功，继续下一步

如果不是，就弹出提示框。然后return  Promise.reject

> mavon-editor    markdown的编辑器

![image-20210706111833449](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210706111833.png)

用一个标签就能够直接实现，markdown编辑

> 解析markdown  解析插件  markdown-it   

md样式插件 用的是github-markdown-css

![image-20210706112433638](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210706112433.png)

渲染

![image-20210706112829836](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210706112829.png)

加样式

在展示的地方，加上一个背景就可以了

![image-20210706113141657](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210706113141.png)



### 后端

[jwt详细](https://github.com/Snailclimb/JavaGuide/blob/master/docs/system-design/authority-certification/JWT%E4%BC%98%E7%BC%BA%E7%82%B9%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md)

> 异常 

 `@ExceptionHandler `  、`@RestControllerAdvice`

```java
package com.xy.common.execption;
/**
 * 捕获全局异常的处理
 */
@RestControllerAdvice
@Slf4j
public class GlobalExceptionHandler {
        /**
     * 根据抛出异常的类进行捕获
     */

    @ResponseStatus(HttpStatus.BAD_REQUEST)
    // 这里是aop切入，异常捕获
    @ExceptionHandler(RuntimeException.class)
    public Result hanler(RuntimeException e){
        log.error("运行时异常");
        return Result.fail(e.getMessage());
    }
    
    @ResponseStatus(HttpStatus.BAD_REQUEST)
    // 这里是aop切入，异常捕获
    @ExceptionHandler(MethodArgumentNotValidException.class)
    public Result hanler(MethodArgumentNotValidException e){
        log.error("实体校验异常：----------------{}", e);
        // 返回实体校验的错误信息message
        BindingResult bindingResult = e.getBindingResult();
        ObjectError objectError = bindingResult.getAllErrors().stream().findFirst().get();
        return Result.fail(objectError.getDefaultMessage());
    }

}

```

> 统一结果封装

自定义了Result类，里面封装根据不同code代码，返回不同信息的方法 。 类包含三个参数code,msg,data

```java
@Data
public class Result implements Serializable {
    // 返回的代码 200正常
    private int code;
    // 返回的信息
    private String msg;
    // 返回的数据
    private Object data;
    public static Result succ(int code,String mess, Object data) {
        Result r = new Result();
        r.setCode(code);
        r.setData(data);
        r.setMsg(mess);
        return r;
    }
    public static Result succ( Object data) {
        return succ(200,"操作成功",data);
    }
    public static Result fail(int code,String mess, Object data) {
        Result r = new Result();
        r.setCode(code);
        r.setData(data);
        r.setMsg(mess);
        return r;
    }
    public static Result fail(String msg) {
        return fail(400,msg,null);
    }
}

```

> 实体校验

实体类里面的校验

```java
@Data
public class LoginDto implements Serializable {
    @NotBlank(message = "用户名不能为空")
    private String username;

    @NotBlank(message = "密码不能为空")
    private String password;
}
```

接口开启校验

```java
 @PostMapping("/save")
public Result save(@Validated @RequestBody User user) {

    String pwd = SecureUtil.md5(user.getPassword());
    user.setPassword(pwd);
    userService.save(user);
    return Result.succ(user);
}
```

> 跨域问题解决

首先实现了一个CorsConfig的配置类，继承webmvc，

```java
/**
 * 解决跨域问题
 */
@Configuration
public class CorsConfig implements WebMvcConfigurer {

    @Override
    public void addCorsMappings(CorsRegistry registry) {
        registry.addMapping("/**")
                .allowedOriginPatterns("*")
                .allowedMethods("GET", "HEAD", "POST", "PUT", "DELETE", "OPTIONS")
                .allowCredentials(true)
                .maxAge(3600)
                .allowedHeaders("*");
    }
}
```



因为我们是自定义了一个jwtFilter来过滤请求，因此，跨域问题在经过滤器之前做了一个预处理.进行跨域

```java
  @Override
    protected boolean preHandle(ServletRequest request, ServletResponse response) throws Exception {

        HttpServletRequest httpServletRequest = WebUtils.toHttp(request);
        HttpServletResponse httpServletResponse = WebUtils.toHttp(response);
        httpServletResponse.setHeader("Access-control-Allow-Origin", httpServletRequest.getHeader("Origin"));
        httpServletResponse.setHeader("Access-Control-Allow-Methods", "GET,POST,OPTIONS,PUT,DELETE");
        httpServletResponse.setHeader("Access-Control-Allow-Headers", httpServletRequest.getHeader("Access-Control-Request-Headers"));
        // 跨域时会首先发送一个OPTIONS请求，这里我们给OPTIONS请求直接返回正常状态
        if (httpServletRequest.getMethod().equals(RequestMethod.OPTIONS.name())) {
            httpServletResponse.setStatus(org.springframework.http.HttpStatus.OK.value());
            return false;
        }
        return super.preHandle(request, response);
    }
```







## RPC框架







### 什么是RPC

RPC（Remote Procedure Call）即远程过程调用，通过名字我们就能看出 RPC 关注的是远程调用而非本地调用。



#### 为什么要 RPC  ？

因为，两个不同的服务器上的服务提供的方法不在一个内存空间，所以，需要通过网络编程才能传递方法调用所需要的参数。并且，方法调用的结果也需要通过网络编程来接收。通过 RPC 可以帮助我们调用远程计算机上某个服务的方法，这个过程就像调用本地方法一样简单。并且！我们不需要了解底层网络编程的具体细节。**RPC 的出现就是为了让你调用远程方法像调用本地方法一样简单**



#### 简单描述一下这个项目

首先一个RPC框架有三个部分，服务提供者(Server)、服务消费端（Client）和注册中心。一个基本过程如下：首先服务提供者先去注册中心，根据方法的完整名称，注册为节点，value就是服务的ip和端口。之后服务消费端（client）以本地调用的方式调用远程服务；客户端 Stub（client stub） 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体（序列化）；客户端 Stub（client stub） 去注册中心，找到远程服务的地址，并将消息发送到服务提供端；服务端 Stub（桩）收到消息将消息反序列化为Java对象；服务端 Stub（桩）根据获得消息中的类、方法、方法参数等信息调用本地的方法；服务端 Stub（桩）得到方法执行结果并将序列化组装成能够进行网络传输的消息体发送至消费方；客户端 Stub（client stub）接收到消息并将消息反序列化为Java对象，这样也就得到了最终结果。



我是使用Zookeeper做的**注册中心**，这块的话，主要之前听说zk比较火，就用的zk做注册中心，不过后来觉得这个方案是有缺陷的，之后准备学习完nacos，将zk替换掉，用nacos作为注册中心。**然后网络传输这块我是用的netty这个框架**，它是一个基于NIO的框架，用它做开发能够简化传统NIO网络编程，而且这个框架的性能比较高。**序列化这块**，我目前的一个选型是kryo，因为目前我的这个框架主要只针对java语言，Kryo 是专门针对Java语言序列化方式并且性能非常好，它是使用 变长存储并使用了字节码生成机制，拥有较高的运行速度和较小的字节码体积。速度快，序列化之后的字节比较少，我还用了gzip对序列化之后的数据进行压缩，大概减少15%的字节数量，进一步加快网络传输的效率。如果后面要做改进的话，应该会再加入可以跨语言的序列化框架，因为RPC的调用在真实业务中更多的是**跨语言的环境下**，我可能到时候会试试hession或者ProtoStuff的序列化方式加入进来慢慢完善。我也有考虑到如果某个服务的访问量特别大的话，我们会将服务部署在多台服务器上，所以加上了负载均衡算法，我目前是用了两个简单的实现，一个是轮询还有随机。

在传输这块，我是自己设计了一个传输协议来进一步压缩数据传输的字节数量和tcp粘包半包的问题。





















### 2.RPC框架实现

#### 1.序列化框架选型

JDK自带的序列化方式一般不会用 ，因为序列化效率低并且部分版本有安全漏洞。比较常用的序列化协议有 hessian、kyro、protostuff。

> 为什么不用jdk自带的

不支持跨语言调用 : 如果调用的是其他语言开发的服务的时候就不支持了。
性能差 ：相比于其他序列化框架性能更低，主要原因是序列化之后的字节数组体积较大，导致传输成本加大。

##### 1.kyro

Kryo是一个高性能的序列化/反序列化工具，由于其变长存储特性并使用了字节码生成机制，拥有较高的运行速度和较小的字节码体积。


由于 Kryo 不是线程安全的。每个线程都应该有自己的 Kryo，Input 和 Output 实例。所以，使用 ThreadLocal 存放 Kryo 对象

Kryo 是专门针对Java语言序列化方式并且性能非常好，如果你的应用是专门针对Java语言的话可以考虑使用



> 优点

Kryo序列化机制比默认的Java序列化机制速度要快，序列化后的数据要更小，**大概是Java序列化机制的1/10。所以Kryo序列化优化以后，可以让网络传输的数据变少，在集群中耗费的内存资源大大减少。**



从序列化后的字节可以看出以下几点：

1、**Kryo序列化后比Hessian小很多**。（kryo优于hessian）

2、由于Kryo没有将类field的描述信息序列化，所以Kryo需要以自己加载该类的filed。这意味着如果该类没有在kryo中注册，或者该类是第一次被kryo序列化时，kryo需要时间去加载该类（hessian优于kryo）

3、由于2的原因，如果该类已经被kryo加载过，那么kryo保存了其类的信息，就可以很快的将byte数组填入到类的field中,而hessian则需要解析序列化后的byte数组中的field信息，对于序列化过的类，kryo优于hessian。



4、**hessian使用了固定长度存储int和long，而kryo则使用的变长，实际中，很大的数据不会经常出现**。(kryo优于hessian)

5、**hessian将序列化的字段长度写入来确定一段field的结束**，**而kryo对于String将其最后一位byte+x70用于标识结束**（kryo优于hessian）



因为我只是想做一个只支持java 的序列化框架，所以选择； Kryo，而且它的性能确实比较强

> 如果想要跨语言可以使用hession

hessian 是一个轻量级的,自定义描述的二进制RPC协议。hessian是一个比较老的序列化实现了，并且同样也是跨语言的。

Dubbo就是用的hession2







#### 1. 技术支持

> Zookpper - 注册中心

- 服务注册 ： 将完整的服务名称 rpcServiceName （类名+版本号）作为根节点 ，子节点是对应的服务地址（ip+端口号）：一个根节点（rpcServiceName）可能会对应多个服务地址（相同服务被部署多份的情况），就使用zk的负载均衡策略取出一个
- 服务发现：我们根据完整的服务名称便可以将对应的服务地址查出来， 查出来的服务地址可能并不止一个。

调用者取到服务器列表后，就可以缓存到自己内部，省得下次再取，当服务器列表发生变化，例如某台服务器宕机下线，或者新加了服务器，ZooKeeper会自动通知调用者重新获取服务器列表

#### 3.Netty

Netty 是一个基于 NIO 的 client-server(客户端服务器)框架，使用它可以快速简单地开发网络应用程序。

它极大地简化并简化了 TCP 和 UDP 套接字服务器等网络编程,并且性能以及安全性等很多方面甚至都要更好。

支持多种协议如 FTP，SMTP，HTTP 以及各种二进制和基于文本的传统协议。

#### 为什么要自定义协议

我们可以自己开发字节数更紧凑，效率更高的协议

```java
 * custom protocol decoder
 * <p>
 * <pre>
 *   0     1     2     3     4        5     6     7     8         9          10      11     12  13  14   15 16
 *   +-----+-----+-----+-----+--------+----+----+----+------+-----------+-------+----- --+-----+-----+-------+
 *   |   magic   code        |version | full length         | messageType| codec|compress|    RequestId       |
 *   +-----------------------+--------+---------------------+-----------+-----------+-----------+------------+
 *   |                                                                                                       |
 *   |                                         body                                                          |
 *   |                                        ... ...                                                        |
 *   +-------------------------------------------------------------------------------------------------------+
 * 4B  magic code（魔数）   1B version（版本）   4B full length（消息长度）    1B messageType（消息类型）// 登录消息还是什么消息
 * 1B compress（压缩类型） 1B codec（序列化类型）    4B  requestId（请求的Id） 发123，但是收是异步的，需要有一个序号对应，请求对应的
 * body（object类型数据）
```

定义了4个字节的魔术，1个字节的版本号，1字节的消息类型，1字节的序列化方式，1个字节的压缩方法，4字节的序列号，4字节的消息长度，

- 消息类型是0，1 用的常量， request 和response
- 序列化方式： 1代表kryo，之后可以扩展hession，0设置成protostuff
- 压缩方式： 1表示GZIP压缩，这个只是用了它的方法，看了一下200个字节大概能压缩20-30，凑齐16位，顺便减少一下传输的字节



### 注册中心

#### Zookeeper不适合做注册中心

CAP理论：

- C：一致性 ，数据更新要同步到整个系统，  （分为强一致性、弱一致性、和最终一致性）	
  - 强一致性：对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。
  - 弱一致性：如果能容忍后续的部分或者全部访问不到，则是弱一致性。
  - 最终一致性：如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。

- A： 服务的可用性，系统可以很好的为用户服务。一般和数据冗余，负载均衡关联

- P：分区容错性，分区之后，仍然能对外提供满足一致性或者可用性的服务。



> CP模型不适合做注册中心

**zookeeper 是 CP**，意味着面对网络分区时，为了保持一致性，他是不可用的。因为 zookeeper 是一个分布式协调系统，如果使用最终一致性（AP）的话，将是一个糟糕的设计。 他的设计都是为了一致性。但是对于服务发现的，可用性是第一位，即使提供旧的数据



比如，这是AP的问题，机器A上服务的ip （1-9），B上有服务的IP（2-10），然后即使数据不一致，顶多也就是1和10上面只有单份流量，分配不均匀而已

 如果出现网络分区了，CP的话，各个区就开始选举leader，节点数少的分区会停止，不可用了，那么服务A 有可能连自己机房的服务都访问不了。。即使没有断掉通信。这就不能接受吧。

> ZK的性能也不适合

大规模服务集群场景中，zookeeper 所有的写操作都是 leader 处理的，在大规模服务注册写请求时，压力巨大，而且 leader 是单点，无法水平扩展。

zookeeper 对每一个写请求，都会写一个事务日志，同时会定期将内存数据镜像dump到磁盘，保持数据一致性和持久性。这也会降低性能



























### 代理模式



你找了小红来帮你问话，小红就可以看作是代理你的代理对象，代理的行为（方法）是问话。

#### 1.静态代理

我们对目标对象的每个方法的增强都是手动完成的，非常不灵活（比如接口一旦新增加方法，目标对象和代理对象都要进行）且麻烦

#### 2.动态代理

相比于静态代理来说，动态代理更加灵活。我们不需要针对每个目标类都单独创建一个代理类，并且也不需要我们必须实现接口，我们可以直接代理实现类( *CGLIB 动态代理机制*)。

**从 JVM 角度来说，动态代理是在运行时动态生成类字节码，并加载到 JVM 中的。**

说到动态代理，Spring AOP、RPC 框架应该是两个不得不的提的，它们的实现都依赖了动态代理。

**动态代理在我们日常开发中使用的相对较小，但是在框架中的几乎是必用的一门技术。学会了动态代理之后，对于我们理解和学习各种框架的原理也非常有帮助。**

就 Java 来说，动态代理的实现方式有很多种，比如 **JDK 动态代理**、**CGLIB 动态代理**等等。

##### jdk动态代理机制和Cglib动态代理

jdk动态代理是通过反射实现的，代理的对象必须实现接口的。  面向接口

cglib动态是利用ASM开源包实现的，是通过修改字节码处理。  通过字节码，底层继承要代理的类。如果类被final关键字修饰，会失败



spring中，如果被代理的对象是个实现类，就会用jdk动态代理完成(默认的)，如果被代理的对象不是实现类，就会用CGlib实现动态代理

性能其实没啥差距，更多的是使用场景的不同





## Mini-SLS

### 1.分库分表

- 自增id 当主键（没有什么用）
- 雪花算法生成的履约单号当做sharding-key
- 分表了，没有分库  -- C32hash，然后取余（扩容问题）
- 一致性hash算法
  - hash弄成一个圈，把20台机器放到圈上，hash%2^32-1然后再去找顺时针看到的第一个机器，就把数据放上面
    - 算法有问题，机器太少没办法均分这个圈，数据倾斜
    - 这时候虚拟节点出现，目的是为了均匀平分这个圈，每个机器虚拟出3个节点，原来有20个机器，那么总共就有60个机器了，能够更好的平分，当走到虚拟节点（A#）的时候，就把数据放到虚拟节点对应的真实机器（A）上
  - 扩容或者减容的时候，并不是数据不需要迁移，而是迁移节点变少了
  - 老hash%20，这时候加了5个机器，所有的数据都要重新hash%25，所有的机器上的数据就都要改变，一致性hash，加了5个机器上去，可能就把他们旁边的机器的数据迁移过来就行
