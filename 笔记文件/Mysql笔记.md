# Mysql

## 1.简单的八股

#### 1.索引

##### 1.索引的优缺点



**优点**：

- 大大加快数据查询的检索速度
- 创建唯一性索引，可以保证数据的唯一性

**缺点**：

- 创建和维护索引会消耗时间
- 索引会耗费一定的空间
- 对表中的数据进行修改时，如果数据有索引，那么索引也要修改，会降低sql的执行效率
- 如果建立聚簇索引，需要的空间更大。如果非聚簇索引很多，一旦聚簇索引改变，所有的非聚簇索引都会跟着改变



##### 2.索引的数据结构

>**1.hash**

hash查询特别快。Mysql为什么不用hash做索引的数据结构？

原因：hash索引不支持顺序和范围查询。 即不能对表中的数据进行排序或者进行范围查询

> **2.B树**

B 树也称 B-树,全称为 **多路平衡查找树** ，B+ 树是 B 树的一种变体。B 树和 B+树中的 B 是 `Balanced` （平衡）的意思。

**B 树& B+树两者有何异同呢？**

- B 树的所有节点既存放键(key) 也存放 数据(data)，而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。
- B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。
- B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。

##### 3.索引类型 和对数据库性能的影响

> 主键索引

一张数据表有只能有一个主键，并且主键不能为 null，不能重复。

特殊的唯一索引。一张表中只能定义一个主键索引，用于标识一条记录





> 二级索引(辅助索引)

二级索引又称为辅助索引，是因为二级索引的**叶子节点存储的数据是主键**。也就是说，通过二级索引，可以定位主键的位置。

- 普通索引
- 唯一索引：数据记录的唯一性
- 联合索引：索引可以覆盖多个数据列，
- 全文索引：倒排索引。可以提升检索效率， like的时候用这个
- 前缀索引(Prefix) ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。



> 聚簇索引和非聚簇索引

聚集索引：

- 优点：查询速度很快
- 缺点：
  - 依赖有序的数据。如果数据是字符串或者UUID这种，插入或者查找速度就比较慢
  - 更新代价要大一些

非聚簇索引：就是二级索引。叶子节点存放的可能是数据的指针也可能是主键

- 优点：更新代价小，因为叶子节点不存放数据
- 缺点：
  - 也依赖有序的数据
  - 可能会二次查询(回表)，就是用**指针的地址**再找一下

> 覆盖索引

**覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 而无需回表查询。**



##### 4、创建索引的注意事项

- 选择合适的字段创建索引
  - 不为null的字段。为null的数据库不好优化
  - 被频繁查询的字段
  - 作为条件的字段
  - 频繁需要排序的字段
- 被频繁更新的字段要慎重建立索引。 一个字段经常被修改，维护成本就很大
- 尽可能的考虑建立联合索引而不是单列索引  。 
  - 因为建立联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。
- 注意避免冗余索引



![image-20210716191739176](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210716191739.png)

```sql
#1.添加 PRIMARY KEY（主键索引）
ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` )

#2.添加 UNIQUE(唯一索引)
ALTER TABLE `table_name` ADD UNIQUE ( `column` )

#3.添加 INDEX(普通索引)
ALTER TABLE `table_name` ADD INDEX index_name ( `column` )

#4.添加 FULLTEXT(全文索引)
ALTER TABLE `table_name` ADD FULLTEXT ( `column`)

#5.添加多列索引
ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` )
```



#### 2.MyISAM 和InnoDB的区别

- 行级锁
  - MyISAM只有表级锁，InnoDB既支持**表级锁**又支持**行级锁**
- 事务
  - MyISAM不支持事务，InnoDb支持事务
- 故障恢复
  - MyISAM不支持数据库异常之后的恢复，InnoDB支持恢复，主要依赖redo log
  - InnoDB使用 **undo log(回滚日志)** 来保证事务的**原子性**。
- 是否支持外键
  - MyISAM不支持外键。InnoDB支持外键
  - 不过我们不建议在数据库层面设置外键，可以在应用层解决（会有数据一致性的问题）。
- 是否支持MVCC
  - MyISAM不支持，InnoDB支持
  - MVCC相当于行级锁的升级，可以减少加锁操作，提高性能

#### 3、锁机制和InnoDB锁算法

表级锁：粒度最大的锁，对当前整个表加锁，实现简单，加锁快，资源消耗少，**触发锁冲突概率最高**，并发度低。

行级锁：粒度最小的锁，对当前操作的行加锁。能大大减少数据库操作的冲突，并发度高，加锁开销大，资源消耗多，会出现死锁。

InnoDB锁算法有：

- Record lock：记录锁，单个行记录上的锁
- Gap lock：间隙锁，锁定一个范围，不包括记录本身
- next-key lock： record + gap。锁定一个范围+记录本身

#### 4、事务

##### 1.什么是事务

数据库事务可以保证，多个对数据库的操作构成一个整体，这些操作要么全部执行，要么都不执行。

##### 2.ACID特性

atomicity**原子性**：事务是最小的执行单位。 要么全部完成，要不不起作用

consistency**一致性**：事务操作前后，数据保持一致。A给B转账50，那么A-50，B就要+50

Insolation**隔离性**：并发执行事务，一个用户的事务不会被其他事务干扰。

durablilty**持久性**：事务提交后，对数据库的改变是持久的。

##### 3.数据事务的实现原理

mysql数据库，使用undo log(回滚)来保证**原子性**，使用redo log(恢复机制)实现持久性，

使用锁来保证隔离性。以上全部满足，就可以保证一致性

#### 4、并发事务带来的问题

1.**脏读**：事务1对数据A进行修改，但是没有提交。此时事务2读取数据A。

​	例子：A=50，事务1修改A = 20，未提交，事务2读取A得到A=20，事务1有问题，回滚事务。那么刚刚事务2读的就是脏数据

2.**丢失修改**：事务1读取到了A=20，事务2读取到了A=20，事务1修改A = A-1，事务2修改A = A-1。最后A的结果是19.事务1的操作丢失

3.**不可重复读**：顾名思义，就是事务1多次读取**同一个数据**，事务2在这个过程中对这个数据进行修改。造成前后数据不一样的情况

4.**幻读**：和不可重复读很像，**区别在于幻读是一次读取多行数据**，事务1先读取多行数据，然后事务2之后插入或者删除了一些数据，这时候事务1又读了多行数据，发现这些数据怎么这么奇怪（多了几行，或者少了几行），发生幻觉了。

#### 5、事务的隔离级别

读取未提交：最低的隔离级别，允许读取尚未提交的数据变更，可能会**脏读**及以上

读取已提交：允许读取并发事务已经提交的数据，只能**防止脏读**

可重复读： 对**同一字段**的多次读取结果是一致的，除非数据是被本身事务修改，可以阻止**脏读和不可重复读**。仍可能会幻读

​		- 就是外面修改了，不会对这个事务里读到的数据有影响，里面一直是5000，外面改成4000提交了，这个事务还是5000

可串行化：最高的隔离级别，完全服从ACID的隔离级别。 所有事务依次执行，事务之间完全不会产生干扰。可以防止上面所有的问题



> 扩展

MySQL INNODB的默认隔离级别是**可重读**，但是mysql可以使用加锁读来避免幻读的产生。这个锁机制就是Next-key Locks

隔离级别越低，事务请求的锁就越少，所以大部分的数据库隔离级别都是**读取已提交**。而INNODB使用**可重读**的性能不会有损失。

InnoDB在分布式事务的情况下会用到**串行化**的隔离级别

#### 6、MVCC

MVCC，全称Multi-Version Concurrency Control，即多版本并发控制

- 当前读：加锁的读
- 快照读：不加锁的select操作就是快照读

MVCC就是为了实现**读-写冲突不加锁**，MVCC多版本并发控制指的是 “维持一个数据的多个版本，使得读写操作没有冲突” 这么一个概念。

##### 数据库并发场景有三种，分别为：

- 读-读：不存在任何问题，也不需要并发控制
- 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读
- 写-写：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失

MVCC可以为数据库解决以下问题：

- 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能
- 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题





MVCC + 悲观锁
MVCC解决读写冲突，悲观锁解决写写冲突

MVCC + 乐观锁
MVCC解决读写冲突，乐观锁解决写写冲突
 这种组合的方式就可以最大程度的提高数据库并发性能，并解决读写冲突，和写写冲突导致的问题

##### 实现原理

里面有三个隐式字段。隐式主键，事务ID，每做一次操作就+1，回滚指针，指向undo log，undo log里面会记录修改的内容。后面没看懂

![image-20210721162528325](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210721162528325.png)





## Mysql45讲

### Mysql基础篇

![image-20210805110050300](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210805110050.png)

#### 1.基础架构

##### 1.连接器

连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。

一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限(这次连接不会生效，下次连接才会生效)

客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。



数据库里面，**长连接**是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。(连接的建立比较复杂，推荐使用长连接。就是连接之后不要断开。**比如jdbc操作的话，我们使用完之后会有一个close()的方法，这就是短连接。如果我们使用数据库连接池，就是长链接**)





MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。长连接累积下来，可能导致内存占用太大，被系统强行杀掉，也即mysql异常重启

> 怎么解决

- 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
- 可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。



##### 2.查询缓存

MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。**但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利**。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。

**MySQL 8.0 版本直接将查询缓存的整块功能删掉了**

##### 3.分析器

- 词法分析：**先确定里面的字符串的意义**，代表什么。select 是查询，表名是T。。等等
- 语法分析：会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。单词错误会报错，一般语法错误会提示第一个出现错误的位置

分析阶段判断语句是否正确，表是否存在，列是否存在等。

##### 4.优化器

经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。

**优化器是在表里面有多个索引的时候，决定使用哪个索引或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。**

优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段

##### 5.执行器

开始执行的时候，要**先判断**一下你对这个表 T **有没有执行查询的权限**，如果没有，就会返回没有权限的错误。(分析器之后也会做权限认证，precheck)

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。



#### 2.日志系统

更新流程会涉及到日志模块。redo log（重做日志）和 binlog（归档日志）。

##### redo log  

------------------------**保证事务的持久性**------------------------

**如果每次修改直接对mysql进行，那么就很慢**。因为要去磁盘进行查找，然后IO更新。于是就先写在log上，然后根据log的记录之后进行修改(其实就是 MySQL 里经常说到的 WAL 技术，**WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘**)

InnoDB 引擎就会先把记录写到 redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。





> redo log 大小

可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作

redo log满了之后又有记录进来，就需要删除之前的记录，也就是擦除

从头开始写，写到末尾就又回到开头循环写

![image-20210805112739101](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210805112739.png)

write pos，当前记录的位置，写完往后移。checkpoint是已经擦除的位置，也是往后移，可以理解成是一个环形栈空间。**write pos 和 checkpoint 之间的**是“粉板”上还空着的部分，可以用来记录新的操作。

可以看成上面的绿色的是空闲的空间。黄色的是已经写了的记录的空间。

如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得**停下来先擦掉一些记录**，把 checkpoint 推进一下。

> crash-safe

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。就是出现问题，我们可以去redo log上找存放的记录，进行一个恢复操作

##### binlog

redo log 是 InnoDB 引擎特有的日志，而 **Server 层也有自己的日志，称为 binlog（归档日志）。**

binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。

binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

![image-20210805121603577](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210805121603.png)



> 怎样让数据库恢复到半个月内任意一秒的状态？

binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会**保存最近半个月的所有 binlog**，同时**系统会定期做整库备份**。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。



==一次误删操作的恢复==

如果我要恢复的话，首先是找到与指定时间最近一次的全量备份(整库备份)，将这个备份恢复到临时库，然后找到备份的时间点，从这个时间点开始，执行bin log里的操作，一直到误删表之前的那个时刻。

这样临时库就是跟误删之前的线上库一样了。



##### redo log 和 bin log 的理解

- redo log
  - 记录的是磁盘上数据的物理变化（记录这个页 “做了什么改动”）。
  - redo log是物理日志，物理日志与存储引擎相关
- binlog
  - 记录的是当时所执行的高级编程语言
  - bin log是逻辑日志，逻辑日志可以跨存储引擎。
  - Binlog有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，记两条，更新前和更新后都有。

#### 3.事务隔离

##### 1.隔离性和隔离级别

隔离性：

- 当数据库上有多个事务同时执行的时候，就可能出现**脏读**（dirty read）、**不可重复读**（non-repeatable read）、**幻读（**phantom read）的问题

隔离级别：

- 读未提交（read uncommitted)：一个事务还没提交时，它做的变更就能被别的事务看到
- 读提交（read committed）：一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在**启动时**看到的数据是一致的。
- 串行化（serializable ）：读取前锁定所有要读读取的数据，当前事务提交前其他事务不允许修改

> 分析

![image-20210806142440115](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806142440.png)

不同隔离级别下得到的值：

- RU。V1 = 2. V2 =V3 =2 。B没有提交，但是结果还是被A看到了
- RC。V1 = 1，V2 =2.V3 = 2。 因为B没提交，所以V1的值还是之前看到的那个。提交之后，这个值才能被A看到
- RR。V1 = 1，V2 = 1，V3 = 2。事务在执行期间看到的数据前后必须是一致的。
- 串行化。V1 = V2 =1 。V3 =2 。但是事务B执行“将 1 改成 2”的时候，**会被锁住**。必须事务A提交之后才能继续执行

实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。**在“可重复读”隔离级别下，这个视图是在事务启动时创建的**，整个事务存在期间都用这个视图。**在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的**。

**注意**：读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

总结来说，存在即合理，每种隔离级别都有自己的使用场景，你要根据自己的业务情况来定。





##### 事务隔离的实现

> 可重复读

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志（undo log）里面就会有类似下面的记录。

![image-20210806143353025](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806143353.png)

当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。

同一条记录在系统中可以存在多个版本，就是**数据库的多版本并发控制（MVCC）**。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。



同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。

回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。

什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。

**最好不用长事务**：

- 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

##### 事务的启动方式

- 显示启动事务，begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。
- set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接
- 

#### 4.索引

##### 常见的索引类型

- 哈希表 （key-value）：把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。多个key经过hash 的值可能相同，使用拉链法处理。
  - 哈希表这种结构适用于只有**等值查询**的场景
- 有序数组：而有序数组在**等值查询和范围查询场景**中的性能就都非常优秀。
  - 有序数组索引只适用于**静态存储引擎**：（更新数据需要挪动很多数据，开销大）。比如你要保存的是 2017 年某个城市的所有人口信息，这类不会再修改的数据。
- 二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。**使用多叉树**。“N 叉”树中的**“N”取决于数据块的大小。**
  - 以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。
- 跳表、LSM 树等数据结构也被用于引擎设计

##### InnoDB的索引模型

InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。**每一个索引在 InnoDB 里面对应一棵 B+ 树。**

![image-20210806144636639](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806144636.png)

主键索引的叶子节点存的是整行数据。主键索引也被称为聚簇索引（clustered index）。

非主键索引的叶子节点内容是主键的值，非主键索引也被称为二级索引（secondary index）。

基于**非主键索引的查询需要多扫描一棵索引树**。如果我们要查的值是主键，就可以不用回表重新扫描，这就是**索引覆盖**。因此，我们在应用中应该尽量使用主键查询。



> 索引维护

B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。根据上图的，如果插入700，只需要在R5的后面插入一个新的值，如果插入的是400就需要逻辑上挪动后面的数据。还可能R5的数据页慢了，B+就需要申请一个新的数据页，然后挪动部分数据过去，这就是**页分裂**。

页分裂：

- 性能会有影响
- 空间利用率降低，原来一个页的数据，现在要放到两个页

有分裂就有合并，如果删除数据之后，利用率比较低，会将数据页做一个合并。



> **自增主键**：

插入新纪录可以不指定ID的值，获取当前最大的ID的值+1.就跟上面直接插入700的例子一样，使用自增主键的话，后面添加新的记录，只需要在最后一个数据页里面**追加**，不需要挪动其他数据。如果这个数据页慢了，就直接申请新的页，将后面的值添加到新的数据页里面，然后更新上面的索引页。**如果申请的数据页比较多，那么索引页还是会进行一个分裂的操作。**

显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。所以，从性能和存储空间方面考量，**自增主键往往是更合理的选择**。因为业务逻辑的字段**不容易保证有序的插入**。而且如果是字符串类型的，一般需要用20个字节。整形数据做主键只需要4个字节，或者长整型的8个字节。就可以一个页放更多的数据了。

有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：

- 只有一个索引
- 该索引必须是唯一索引。



##### 最左前缀匹配

**联合索引**：

![image-20210806151823599](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806151823.png)

这是一个name和age的联合索引，顺序是按照name来排列的。然后当name相同，才是按照age进行一个排序。



如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是"where name like ‘张 %’"。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。

**只要满足最左前缀，就可以利用索引来加速检索**。最左前缀可以是联合索引的最左 N 个字段

>在建立联合索引的时候，如何安排索引内的字段顺序。

所以当已经有了 (a,b) 这个联合索引后，**一般就不需要单独在 a 上建立索引了**。

第一原则是，**如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的**。

##### 索引下推

**这里建立了name和age的联合索引**

select * from tuser where name like '张%' and age=10 and ismale=1;

你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。

MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

 **其实就是索引空间里面存放着有name和age，然后查找name之后，直接用age的值进行比较，然后不满足的，就不回表比对**

![image-20210806152626965](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210806152627.png)







#### 5.各种锁

**DML**：数据库管理语言，如select、update这些

**DDL**：数据库定义语言，这个是对表的操作，比如给表加字段的命令

##### 全局锁

全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的语句会被阻塞(DML、DDL)。

全局锁的典型使用场景是，做**全库逻辑备份**。在备份过程中整个库完全处于只读状态。这是比较危险的：

- 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；
- 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。



> 我们可以不用加锁，也能实现的备份

在可重复读隔离级别下开启一个事务。能够拿到一致性视图

官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。

但是有的数据库引擎不支持事务，或者这个隔离级别。所以需要使用FTWRL。



##### 表锁

MySQL 里面表级别的锁有两种：一种是**表锁**，一种是**元数据锁（meta data lock，MDL)**。

表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。

表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。



**另一类表级的锁是 MDL（metadata lock)**

当对一个表做增删改查操作的时候(DML语句)，加 MDL 读锁；当要对**表做结构变更**操作的时候(DDL语句)，加 **MDL 写锁**。

- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。



![image-20210818183031032](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210818183031.png)

我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。之后所有要在表 t 上新申请 MDL 读锁的请求**也会被 session C 阻塞**。就都被锁住，等于这个表现在完全不可读写了。

==为什么C锁住了，后面的读锁也都锁住==是因为申请MDL锁的时候，会形成一个队列，队列中**写锁的获取的优先级高于读锁**-。于是写锁如果等待了，后面的读锁就抢不过写锁，也会阻塞住。

> ==实际操作中，必须把D提交了，C才能继续执行，怎么会出现插队的情况？？？== 

是因为mysql的online ddl机制，DDL(更改表结构的语句)，ddl执行的时候，如果锁住表就会严重影响性能，不锁表，又不好解决DML语句的影响，于是有了上面这个机制。 这个会让DDL有个锁降级的过程，如下：

- 事务拿到MDL写锁
- 降级成MDL读锁
- 申请一块空间，开始改变表结构，填数据(做DDL操作)
- 升级为MDL写锁，然后把上述表替换之前的表
- 释放MDL写锁

可以知道，在锁降级之后，事务D就会拿到MDL的读锁，因此，我们升级回MDL写锁，就会被阻塞住。

> 如何安全的给小表加字段

![image-20210818184217134](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210818184217.png)



##### 行锁

MySQL 的行锁是在引擎层由各个引擎自己实现的。不是所有的引擎都支持行锁，MyISAM 引擎就不支持行锁，意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。



在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时(commit)才释放。这个就是**两阶段锁协议**。



知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行**，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。**



有一个业务的操作：

- 1 从顾客 A 账户余额中扣除电影票价；
- 2 给影院 B 的账户余额增加这张电影票价；
- 3 记录一条交易日志。

这时候另一个事务是，顾客C要在影院B买票，那么就和语句2产生锁冲突了，所以我们要把语句2放在最后处理，也就是事务语句位置为3,1,2.

> 死锁和死锁检查

![](https://xy-picgo.oss-cn-shenzhen.aliyuncs.com/20210818211253.png)

上面会出现死锁的情况，出现死锁有两种解决办法：

- 直接进入等待，一直到超时，超时时间可以通过参数 innodb_lock_wait_timeout 来设置。
- 发起死锁检查。发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。

在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。又不可能直接把这个时间设置太小，比如 1s。这样的话如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。

**一般都采用第二种方式**：

加入这样一个过程，每个事务被锁的时候，都要查看它依赖的线程有没有被别人锁住，然后判断是否出现死锁。如果是所有事务都更行同一行(热点数据),那么每个新来的线程都会被堵住，然后判断是不是自己导致死锁，假设有1000个并发的，就是1000*1000 = 100w的检测量级。这并没有发送死锁，但是会消耗大量的CPU资源。



>怎么解决由这种热点行更新导致的性能问题呢？

1.如果确定业务一定不会出现死锁，可以关闭死锁检测。业务设计的时候一般不会把死锁当做一个严重错误，毕竟**出现死锁了，就回滚**，然后通过业务重试一般就没问题了，这是**业务无损的**。而**关掉死锁检测**意味着可能会**出现大量的超时**，这是**业务有损的**。

2.控制并发度。并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现。**对于相同行的更新，在进入引擎之前排队。**



### 12.Mysql实践篇

11